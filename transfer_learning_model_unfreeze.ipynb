{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning for Melanoma Classification using EfficientNetB0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up for importing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import random_state\n",
    "random_state = random_state() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_results = True\n",
    "on_cluster = True # there is some issue with h5py in the local env but it works on cluster so w/e\n",
    "export_folder = f'results/Transfer_Learning/161122-unfreeze50'\n",
    "\n",
    "date_format = \"%d%m%Y%H%M%S\" # timestamp format in exported files\n",
    "if export_results:\n",
    "    import datetime\n",
    "    import os\n",
    "    if not os.path.exists(export_folder): \n",
    "      os.makedirs(export_folder)\n",
    "      print(\"Created new directory %s\" %export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolution Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pixel = 224 # default : 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timer\n",
    "\n",
    "Start the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_data = True\n",
    "base_path = \"data/30\" \n",
    "current_train_melanoma_percentage = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get image paths\n",
    "For developing models on the cluster the max_images parameter should be removed. Instead call the method get_all_img_paths(img_folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import get_all_img_paths, get_img_paths\n",
    "\n",
    "img_folder_train = base_path + \"/train\" + (\"_downsampled\" if downsampled_data else \"\")\n",
    "img_folder_test = base_path + \"/test\" + (\"_downsampled\" if downsampled_data else \"\")\n",
    "\n",
    "max_images_train = int(13653*1)\n",
    "max_images_test = int(5804*1)\n",
    "\n",
    "img_paths_train = get_img_paths(img_folder_train, max_images_train) \n",
    "img_paths_test = get_img_paths(img_folder_test, max_images_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Loads the images specified in img_paths into a data frame. This includes resizing the images and flattening them into an array and may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import load_train_test\n",
    "\n",
    "groundtruth_file_train = base_path + \"/ISIC_2020_2019_train\" + (\"_downsampled\" if downsampled_data else \"\") + \".csv\" \n",
    "groundtruth_file_test = base_path + \"/ISIC_2020_2019_test\" + (\"_downsampled\" if downsampled_data else \"\") + \".csv\"\n",
    "\n",
    "\n",
    "# available options\n",
    "options = [\"sequential\", # first load train, then load test\n",
    "           \"parallel_train_test\", # load train and test parallel (load data within train and test sequential)\n",
    "           \"sequential_train_test_parallel_chunks\", # load first train, then test, but load the data within the sets parallel\n",
    "           \"parallel_fusion\" # run train and test parallel and parallely load data with train and test \n",
    "          ]\n",
    "\n",
    "# chose an option\n",
    "option = \"parallel_fusion\"\n",
    "\n",
    "df_train, df_test = load_train_test(img_paths_train, groundtruth_file_train, \n",
    "                                    img_paths_test, groundtruth_file_test, \n",
    "                                    option, img_pixel);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into target and predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import split_predictors_target\n",
    "\n",
    "X_train, y_train = split_predictors_target(df_train) \n",
    "X_test, y_test = split_predictors_target(df_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import unflatten_images_df\n",
    "\n",
    "# this should rlly be optimized so that this step is no longer necessary lol\n",
    "X_train = unflatten_images_df(X_train, img_pixel=img_pixel)\n",
    "X_test = unflatten_images_df(X_test, img_pixel=img_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete some no longer needed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del(df_train)\n",
    "del(df_test)\n",
    "del(img_paths_train)\n",
    "del(img_paths_test)\n",
    "del(img_folder_train)\n",
    "del(img_folder_test)\n",
    "del(groundtruth_file_train)\n",
    "del(groundtruth_file_test)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions for training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau \n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "melanoma_weight = (1/current_train_melanoma_percentage)/2\n",
    "class_weight = {0: 1.,\n",
    "                1: melanoma_weight,}\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "            \n",
    "def unfreeze_model(model, base_model, num_layers = 20): # a function to unfreeze num_layers layers of a model\n",
    "    model.trainable = True\n",
    "    for layer in base_model.layers[:-num_layers]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "            \n",
    "def compile_model(model, lr=0.001):\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Nadam(lr), metrics=['accuracy', Recall(name=\"recall\")])\n",
    "    \n",
    "def fit_model(model):\n",
    "    callbacks = [EarlyStopping(patience=10), lr_scheduler]\n",
    "    if export_results:\n",
    "        filename = export_folder + \"/training_log_\"+datetime.datetime.now().strftime(date_format)+\".csv\"\n",
    "        callbacks.append(CSVLogger(filename, separator=\",\", append=False))\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        batch_size=32, epochs=100,\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def eval_model(model, history, name):\n",
    "    display_history(history,name)\n",
    "    gc.collect()\n",
    "    display_confusion_matrix(model,name)\n",
    "\n",
    "def display_history(history, name):\n",
    "    _, axs = pyplot.subplots(3, 1, figsize=(20,15))\n",
    "\n",
    "    # plot loss during training\n",
    "    axs[0].plot(history_frozen.history['loss'], label='train')\n",
    "    axs[0].plot(history_frozen.history['val_loss'], label='test')\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # plot accuracy during training\n",
    "    axs[1].plot(history_frozen.history['accuracy'], label='train')\n",
    "    axs[1].plot(history_frozen.history['val_accuracy'], label='test')\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    # plot recall during training\n",
    "    axs[2].plot(history_frozen.history['recall'], label='train')\n",
    "    axs[2].plot(history_frozen.history['val_recall'], label='test')\n",
    "    axs[2].set_title(\"Recall\")\n",
    "    axs[2].legend()\n",
    "\n",
    "    if export_results:\n",
    "        pyplot.savefig(export_folder + \"/loss_and_accuracy_during_training_\"+name+\"_\"+datetime.datetime.now().strftime(date_format)+\".png\")\n",
    "\n",
    "    pyplot.show()\n",
    "    \n",
    "def display_confusion_matrix(mode, name):\n",
    "    y_pred_continuous = model.predict(X_test)\n",
    "    y_pred_discrete = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    y_pred = y_pred_discrete\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    print(f'\\nClassification_report=\\n{report}')\n",
    "\n",
    "    if export_results:\n",
    "        file = open(export_folder + \"/classification_report_\"+name+\"_\"+datetime.datetime.now().strftime(date_format)+\".txt\", 'w')\n",
    "        file.write(report)\n",
    "        file.close()\n",
    "        \n",
    "    class_names = [\"no melanoma\", \"melanoma\"]\n",
    "\n",
    "    cf = confusion_matrix(y_test, y_pred)\n",
    "    plot = sns.heatmap(cf, annot= True, fmt=\".0f\",\n",
    "               xticklabels = class_names,\n",
    "               yticklabels = class_names)\n",
    "    plot.set(xlabel='Prediction', ylabel='Actual')\n",
    "\n",
    "    if export_results:\n",
    "        plot.get_figure().savefig(export_folder + '/confusion_matrix_' +name + \"_\" + datetime.datetime.now().strftime(date_format) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.layers import Convolution2D, Dense, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# build model\n",
    "base_model = efn.EfficientNetB0(input_shape=(img_pixel ,img_pixel , 3),weights='imagenet',include_top=False)\n",
    "base_model.trainable = False # freezing all the layers\n",
    "\n",
    "print(base_model.layers)\n",
    "\n",
    "add_model = keras.Sequential(base_model)\n",
    "\n",
    "add_model.add(GlobalAveragePooling2D(name=\"avg_pool\"))\n",
    "add_model.add(BatchNormalization())\n",
    "top_dropout_rate = 0.2\n",
    "add_model.add(Dropout(top_dropout_rate, name=\"top_dropout\"))\n",
    "add_model.add(Dense(1, activation='sigmoid'))\n",
    "model = add_model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model (while still frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first training\n",
    "compile_model(model)\n",
    "history_frozen = fit_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "eval_model(model, history_frozen, \"frozen\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze and retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_model(model, base_model, 20)\n",
    "print('done1')\n",
    "gc.collect()\n",
    "compile_model(model, 0.0001) # extra low learning rate to avoid overfitting\n",
    "print('done2')\n",
    "gc.collect()\n",
    "history = fit_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "eval_model(model, history, \"unfrozen\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model\n",
    "if export_results and on_cluster:\n",
    "    model.save(export_folder + \"/model_\"+datetime.datetime.now().strftime(date_format)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timer\n",
    "Stop the timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = time.time()\n",
    "print(f'It took {stop - start} s to load the data and train the model')\n",
    "\n",
    "if export_results:\n",
    "    f = open(f'{export_folder}/overall_time.txt', 'w')\n",
    "    f.write(f'Time it took : {stop - start} s')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# _, train_acc, train_recall = model.evaluate(X_train, y_train)\n",
    "# _, test_acc, test_recall  = model.evaluate(X_test, y_test)\n",
    "\n",
    "# print('Accuracy\\tTrain: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# print('Recall\\tTrain: %.3f, Test: %.3f' % (train_recall, test_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict test set (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_continuous = model.predict(X_test)\n",
    "y_pred_discrete = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "y_pred = y_pred_discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display images and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import display_results\n",
    "\n",
    "plt_all = display_results(X_test, y_pred, y_test, 15, img_pixel, flat=False)\n",
    "\n",
    "if export_results:\n",
    "    plt_all.savefig(export_folder + \"/classification_results_\"+datetime.datetime.now().strftime(date_format)+\".png\")\n",
    "\n",
    "plt_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display wrongly classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import display_interesting_results\n",
    "\n",
    "plt_wrong = display_interesting_results(X_test, y_pred, y_test, img_pixel=img_pixel, flat=False)\n",
    "\n",
    "if export_results:\n",
    "    plt_wrong.savefig(export_folder + \"/incorrect_classification_results_\"+datetime.datetime.now().strftime(date_format)+\".png\")\n",
    "    \n",
    "plt_wrong.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
